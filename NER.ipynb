{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc611cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import RobertaTokenizerFast\n",
    "import evaluate\n",
    "import torch\n",
    "from statistics import mean\n",
    "from pprint import pprint\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfca6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Читаем Excel ===\n",
    "df = pd.read_csv(\"train.csv\", sep=';')\n",
    "df_percent = pd.read_csv(\"train_new_percent.csv\", sep=';')\n",
    "df_t = pd.read_csv(\"submission.csv\", sep=';')\n",
    "\n",
    "# Список меток\n",
    "labels = ['O', 'B-BRAND', 'I-BRAND', 'B-TYPE', 'I-TYPE',\n",
    "          'B-PERCENT', 'I-PERCENT', 'B-VOLUME', 'I-VOLUME']\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for i, l in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf220d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(train_path: str) -> Dataset:\n",
    "    # Список меток\n",
    "    labels = ['O', 'B-BRAND', 'I-BRAND', 'B-TYPE', 'I-TYPE',\n",
    "              'B-PERCENT', 'I-PERCENT', 'B-VOLUME', 'I-VOLUME']\n",
    "    label2id = {l: i for i, l in enumerate(labels)}\n",
    "\n",
    "    df = pd.read_csv(train_path, sep=';')\n",
    "\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[0]\n",
    "        spans = ast.literal_eval(row[1])  # список [(start, end, label), ...]\n",
    "\n",
    "        # посимвольные метки\n",
    "        char_labels = [\"O\"] * len(text)\n",
    "        for start, end, tag in spans:\n",
    "            end = min(end, len(text))  # не выходим за границу\n",
    "            for i in range(start, end):\n",
    "                char_labels[i] = tag\n",
    "\n",
    "        # преобразуем в токены\n",
    "        tokens = text.split()\n",
    "        token_labels = []\n",
    "        offset = 0\n",
    "        for token in tokens:\n",
    "            token_len = len(token)\n",
    "            token_span_labels = char_labels[offset:offset + token_len]\n",
    "            tag = \"O\"\n",
    "            for l in token_span_labels:\n",
    "                if l != \"O\":\n",
    "                    tag = l\n",
    "                    break\n",
    "            token_labels.append(tag)\n",
    "            offset += token_len + 1  # +1 за пробел\n",
    "\n",
    "        token_labels = [l if l != \"0\" else \"O\" for l in token_labels]\n",
    "        data.append({\n",
    "            \"tokens\": tokens,\n",
    "            \"labels\": [label2id[l] for l in token_labels]\n",
    "        })\n",
    "\n",
    "    return Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "051c139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = make_dataset(\"train.csv\")\n",
    "dataset_new = make_dataset(\"train_new_percent.csv\")\n",
    "\n",
    "dataset_combined = concatenate_datasets([dataset, dataset_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf784bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Загружаем токенайзер и модель ===\n",
    "model_name = \"DeepPavlov/rubert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb00d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(example[\"tokens\"],\n",
    "                          is_split_into_words=True,\n",
    "                          truncation=True,\n",
    "                          padding=\"max_length\",\n",
    "                          max_length=128)\n",
    "\n",
    "    word_ids = tokenized.word_ids(batch_index=0)  # соответствие токен -> слово\n",
    "    label_ids = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)  # игнорируем паддинг\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(example[\"labels\"][word_idx])\n",
    "        else:\n",
    "            # Для сабтокенов ставим I-* если это не \"O\"\n",
    "            label_ids.append(\n",
    "                example[\"labels\"][word_idx]\n",
    "                if labels[example[\"labels\"][word_idx]] != \"O\"\n",
    "                else -100\n",
    "            )\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized[\"labels\"] = label_ids\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "624f2fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████| 27524/27524 [00:06<00:00, 4396.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "encoded_dataset = dataset_combined.map(tokenize_and_align_labels)\n",
    "\n",
    "#encoded_dataset_t = dataset_t.map(tokenize_and_align_labels)\n",
    "\n",
    "#combined_dataset = DatasetDict({\n",
    "#    \"train\": encoded_dataset,\n",
    "#    \"test\": encoded_dataset_t\n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f051f595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# === 4. Готовим модель ===\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74ee86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. Тренировка ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_rubert\",\n",
    "    #evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=500,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ad9dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8e83358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3081667/1048177495.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# === 6. Trainer ===\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset,\n",
    "    #eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b905ff59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12930' max='12930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12930/12930 20:36, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.545900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.093900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.044100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12930, training_loss=0.04043487360080083, metrics={'train_runtime': 1236.8112, 'train_samples_per_second': 667.62, 'train_steps_per_second': 10.454, 'total_flos': 5.39429025522647e+16, 'train_loss': 0.04043487360080083, 'epoch': 30.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efbd2fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_model_test_3/tokenizer_config.json',\n",
       " './saved_model_test_3/special_tokens_map.json',\n",
       " './saved_model_test_3/vocab.txt',\n",
       " './saved_model_test_3/added_tokens.json',\n",
       " './saved_model_test_3/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Не забываем увеличивать каждый раз номер, чтобы сохранить все вариации\n",
    "model.save_pretrained(\"./saved_model_test_3\")\n",
    "tokenizer.save_pretrained(\"./saved_model_test_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f19986d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./saved_model_test_3\"  # путь к сохранённой модели\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_path)\n",
    "model1 = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.to(device)\n",
    "model1.eval()  # переводим в режим инференса\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "414e055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ner(text, model, tokenizer):\n",
    "    # Токенизация\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    #tokens = text.split(\" \")\n",
    "    #inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Предсказание\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "    # Сопоставление токенов с метками\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze())\n",
    "    labels = [model.config.id2label[i] for i in pred_ids]\n",
    "\n",
    "    # Игнорируем спец-токены [CLS] и [SEP]\n",
    "    tokens_labels = [(t, l) for t, l in zip(tokens, labels) if t not in (\"[CLS]\", \"[SEP]\")]\n",
    "\n",
    "    # Склеиваем подслова\n",
    "    merged_tokens, merged_labels = [], []\n",
    "    for t, l in tokens_labels:\n",
    "        if t.startswith(\"##\") or (merged_tokens and t in [\".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"—\", \"–\", \"...\"]):\n",
    "            merged_tokens[-1] += t[2:]\n",
    "        else:\n",
    "            merged_tokens.append(t)\n",
    "            merged_labels.append(l)\n",
    "\n",
    "    return list(zip(merged_tokens, merged_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a031313b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "молоко          -> B-TYPE\n",
      "3               -> B-PERCENT\n",
      "2               -> B-PERCENT\n",
      "%               -> I-PERCENT\n"
     ]
    }
   ],
   "source": [
    "text1 = \"b-brand с клубникой\"\n",
    "text = \"молоко 3.2%\"\n",
    "result = predict_ner(text, model1, tokenizer1)\n",
    "\n",
    "for token, label in result:\n",
    "    print(f\"{token:15} -> {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0692dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    # Восстанавливаем word-level метки, игнорируя паддинги (-100)\n",
    "    for pred, lab in zip(predictions, labels):\n",
    "        cur_preds = []\n",
    "        cur_labels = []\n",
    "        for p_i, l_i in zip(pred, lab):\n",
    "            if l_i != -100:\n",
    "                cur_preds.append(id2label[p_i])\n",
    "                cur_labels.append(id2label[l_i])\n",
    "        true_predictions.append(cur_preds)\n",
    "        true_labels.append(cur_labels)\n",
    "\n",
    "    # Вычисляем метрики seqeval\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # Собираем micro метрики (overall_*)\n",
    "    metrics = {\n",
    "        \"precision_micro\": results.get(\"overall_precision\", 0.0),\n",
    "        \"recall_micro\": results.get(\"overall_recall\", 0.0),\n",
    "        \"f1_micro\": results.get(\"overall_f1\", 0.0),\n",
    "        \"accuracy\": results.get(\"overall_accuracy\", 0.0),\n",
    "    }\n",
    "\n",
    "    # Macro-F1 = среднее F1 по всем существующим классам\n",
    "    f1_per_class = [\n",
    "        results[label].get(\"f1\", 0.0)   # <- используем ключ 'f1', не 'f1-score'\n",
    "        for label in results\n",
    "        if label not in [\"overall_precision\",\"overall_recall\",\"overall_f1\",\"overall_accuracy\"]\n",
    "    ]\n",
    "    metrics[\"f1_macro\"] = mean(f1_per_class) if f1_per_class else 0.0\n",
    "\n",
    "    # F1/precision/recall по каждому классу\n",
    "    for label in results:\n",
    "        if label not in [\"overall_precision\",\"overall_recall\",\"overall_f1\",\"overall_accuracy\"]:\n",
    "            metrics[f\"{label}_precision\"] = results[label].get(\"precision\", 0.0)\n",
    "            metrics[f\"{label}_recall\"] = results[label].get(\"recall\", 0.0)\n",
    "            metrics[f\"{label}_f1\"] = results[label].get(\"f1\", 0.0)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96dfa7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03e93088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 4135.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset_t = dataset_t.map(tokenize_and_align_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "722f3a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1425600/2974128623.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_t = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='341' max='341' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [341/341 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_BRAND_f1': 0.9382334774552191,\n",
      " 'eval_BRAND_precision': 0.9376543209876543,\n",
      " 'eval_BRAND_recall': 0.9388133498145859,\n",
      " 'eval_PERCENT_f1': 1.0,\n",
      " 'eval_PERCENT_precision': 1.0,\n",
      " 'eval_PERCENT_recall': 1.0,\n",
      " 'eval_TYPE_f1': 0.9763793725207357,\n",
      " 'eval_TYPE_precision': 0.9732207045291158,\n",
      " 'eval_TYPE_recall': 0.9795586107091172,\n",
      " 'eval_VOLUME_f1': 0.9473684210526316,\n",
      " 'eval_VOLUME_precision': 0.9,\n",
      " 'eval_VOLUME_recall': 1.0,\n",
      " 'eval_accuracy': 0.9651982888195167,\n",
      " 'eval_f1_macro': 0.9654953177571466,\n",
      " 'eval_f1_micro': 0.9677509228947552,\n",
      " 'eval_loss': 0.27997273206710815,\n",
      " 'eval_model_preparation_time': 0.0027,\n",
      " 'eval_precision_micro': 0.9651292025562657,\n",
      " 'eval_recall_micro': 0.9703869255482609,\n",
      " 'eval_runtime': 3.5799,\n",
      " 'eval_samples_per_second': 761.481,\n",
      " 'eval_steps_per_second': 95.255}\n"
     ]
    }
   ],
   "source": [
    "trainer_t = Trainer(\n",
    "    model=model1,\n",
    "    tokenizer=tokenizer1,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Прогоняем тестовый датасет\n",
    "results = trainer_t.evaluate(encoded_dataset[\"test\"])\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b17eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_csv(input_csv, output_csv, model, tokenizer, device=\"cuda\", batch_size=512):\n",
    "    df = pd.read_csv(input_csv, sep=';')\n",
    "    texts = df.iloc[:, 0].tolist()  # все тексты в список\n",
    "    \n",
    "    results = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_spans = predict_ner_spans_batch(batch_texts, model, tokenizer, device=device)\n",
    "        \n",
    "        for text, spans in zip(batch_texts, batch_spans):\n",
    "            results.append({\"sample\": text, \"annotation\": spans})\n",
    "    \n",
    "    pd.DataFrame(results).to_csv(output_csv, index=False, encoding=\"utf-8\", sep=';')\n",
    "    print(f\"✅ Результаты сохранены в {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "476be5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ner_spans_batch(texts, model, tokenizer, device=\"cuda\"):\n",
    "    # Токенизация батча\n",
    "    inputs = tokenizer(\n",
    "        texts, \n",
    "        return_tensors=\"pt\", \n",
    "        return_offsets_mapping=True, \n",
    "        padding=True, \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    offset_mappings = inputs.pop(\"offset_mapping\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Предсказание для всего батча\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "    \n",
    "    all_batch_spans = []\n",
    "    \n",
    "    # Обрабатываем каждый пример в батче по отдельности\n",
    "    for i in range(len(texts)):\n",
    "        text = texts[i]\n",
    "        # Берем данные для одного примера - ВСЕ на CPU\n",
    "        input_ids = inputs[\"input_ids\"][i].cpu()\n",
    "        attention_mask = inputs[\"attention_mask\"][i].cpu()\n",
    "        offset_mapping = offset_mappings[i]  # уже на CPU\n",
    "        preds = pred_ids[i]\n",
    "        \n",
    "        # Убираем паддинг\n",
    "        valid_indices = attention_mask == 1\n",
    "        input_ids = input_ids[valid_indices]\n",
    "        offset_mapping = offset_mapping[valid_indices]\n",
    "        preds = preds[:len(input_ids)]  # обрезаем по длине без паддинга\n",
    "        \n",
    "        # Конвертируем в списки как в оригинальном коде\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        labels = [model.config.id2label[pred] for pred in preds]\n",
    "        offsets = offset_mapping.tolist()\n",
    "        \n",
    "        # Игнорируем спец-токены [CLS] и [SEP] - ВАША ЛОГИКА\n",
    "        tokens_data = []\n",
    "        for tok, lab, (s, e) in zip(tokens, labels, offsets):\n",
    "            if tok not in (\"[CLS]\", \"[SEP]\"):\n",
    "                tokens_data.append((tok, lab, (s, e)))\n",
    "        \n",
    "        \"\"\"\"\n",
    "        # Склеиваем сабтокены - ВАША ЛОГИКА\n",
    "        merged_tokens = []\n",
    "        for tok, lab, (s, e) in tokens_data:\n",
    "            if tok.startswith(\"##\") or (merged_tokens and tok in [\".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"—\", \"–\", \"...\"]):\n",
    "                prev_tok, prev_lab, (ps, pe) = merged_tokens[-1]\n",
    "                merged_tokens[-1] = (prev_tok + tok[2:], prev_lab, (ps, e))\n",
    "            # Склеиваем %, если он сразу после числа (в тексте нет пробела)\n",
    "            elif tok == \"%\" and prev_tok.replace(\".\", \"\").isdigit():\n",
    "                if texts[i][pe:s] == \"\":\n",
    "                    merged_tokens[-1] = (prev_tok + tok, prev_lab, (ps, e))\n",
    "                    continue\n",
    "            else:\n",
    "                merged_tokens.append((tok, lab, (s, e)))\n",
    "        \"\"\"\n",
    "                \n",
    "        merged_tokens = []\n",
    "        for tok, lab, (s, e) in tokens_data:\n",
    "            if merged_tokens:\n",
    "                prev_tok, prev_lab, (ps, pe) = merged_tokens[-1]\n",
    "\n",
    "                # 1️⃣ Сабтокены WordPiece\n",
    "                if tok.startswith(\"##\"):\n",
    "                    merged_tokens[-1] = (prev_tok + tok[2:], prev_lab, (ps, e))\n",
    "                    continue\n",
    "\n",
    "                # 2️⃣ Склеиваем %, если он сразу после числа без пробела в тексте\n",
    "                if tok == \"%\" and prev_tok.replace(\".\", \"\").isdigit():\n",
    "                    # смотрим, есть ли символы между токенами в исходном тексте\n",
    "                    if text[pe:s] == \"\":  # если между токенами нет пробела\n",
    "                        merged_tokens[-1] = (prev_tok + tok, prev_lab, (ps, e))\n",
    "                        continue\n",
    "\n",
    "                # 3️⃣ Пунктуация (как раньше)\n",
    "                if tok in [\".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"—\", \"–\", \"...\"]:\n",
    "                    merged_tokens[-1] = (prev_tok + tok, prev_lab, (ps, e))\n",
    "                    continue\n",
    "\n",
    "            # 4️⃣ Если ничего не склеилось — добавляем новый токен\n",
    "            merged_tokens.append((tok, lab, (s, e)))\n",
    "        \n",
    "        # Преобразуем в спаны - ВАША ЛОГИКА\n",
    "        spans = []\n",
    "        current_span = None\n",
    "        for tok, lab, (s, e) in merged_tokens:\n",
    "            if current_span:\n",
    "                if current_span[2] == lab:\n",
    "                    if \"B-\" in lab:\n",
    "                        current_span[1] = e\n",
    "                        continue\n",
    "            if current_span:\n",
    "                spans.append(tuple(current_span))\n",
    "            current_span = [s, e, lab]\n",
    "            \n",
    "        if current_span:\n",
    "            spans.append(tuple(current_span))\n",
    "        \n",
    "        all_batch_spans.append(spans)\n",
    "    \n",
    "    return all_batch_spans\n",
    "\n",
    "# Сохраняем оригинальную функцию для обратной совместимости\n",
    "def predict_ner_spans(text, model, tokenizer, device=\"cuda\"):\n",
    "    return predict_ner_spans_batch([text], model, tokenizer, device=device)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c90a96ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Результаты сохранены в submission_new.csv\n",
      "CPU times: user 933 ms, sys: 4.07 ms, total: 937 ms\n",
      "Wall time: 941 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_inference_csv(\"submission.csv\", \"submission_new.csv\", model1, tokenizer1, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b96c4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
